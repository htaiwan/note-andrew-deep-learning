## Week 2. Optimization algorithms

### 2.1 Mini-batch gradient descent
> [2.1.1  為什麼需要Mini-batch gradient descent?](#2.1.1)
> 
> [2.1.2  Batch vs Stochastic vd Mini-batch gradient descent](#2.1.2)
> 

### 2.2 Understanding mini-batch gradient descent

> [2.2.1  圖解mini-batch gradient descent](#2.2.1)
> 
> [2.2.2  程式碼mini-batch gradient descent](#2.2.2)
> 
> [2.2.3  如何選擇適當mini-batch size](#2.2.3)

### 2.3 Exponentially weighted averages

> [2.3.1  為什麼要了解Exponentially weighted averages?](#2.3.1)
> 
> [2.3.2  什麼是Exponentially weighted averages?](#2.3.2)
> 

### 2.4 Understanding exponentially weighted averages

> [2.4.1  exponentially是怎麼來的?](#2.4.1)
> 
> [2.4.2  exponentially weighted averages的優勢?](#2.4.2)
>  

### 2.5 Bias correction in exponentially weighted averages

> [2.5.1  為什麼需要了解Bias correction?](#2.5.1)
> 
> [2.5.2  exponentially weighted averages存在的不準確在哪裡?](#2.5.2)
> 
> [2.5.3  Bias correction是如何改善不準確?](#2.5.3)

### 2.6 Gradient descent with momentum

> [2.6.1  momentum的概念](#2.6.1)
> 
> [2.6.2  momentum的實作](#2.6.2)

### 2.7 RMSprop

> [2.7.1  RMSprop的概念](#2.7.1)

### 2.8 Adam optimization algorithm

> [2.8.1  Adam的概念](#2.8.1)

### 2.9 Learning rate decay

> [2.9.1  Learning rate decay的概念](#2.9.1)

### 2.10 The problem of local optimal

> [2.10.1 瞭解local optimal跟saddle point](#2.10.1)


---

## Answers: 

<h3 id="2.1.1">2.1.1  為什麼需要Mini-batch gradient descent?</h3>

> * 因為深度學習，是一種實驗科學，需要不斷的反覆實驗修正。
> * **Mini-batch gradient descent**可以快速訓練，得到驗證結果。

<h3 id="2.1.2">2.1.2  Batch vs Stochastic vd Mini-batch gradient descent?</h3>

> * **Vectorizqtion**: 加速多樣本的計算，不需要loop，提高效率。
> * 1. **Batch gradient descent**: 是針對全部樣本，進行backward prop。
> 
> > * Vectorizqtion雖然能提高效率，但對於超大數量的樣本，每次得到損失值仍然費時。
> 
> * 2. **Stochastic gradient descent**: 是針對單一樣本。
> 
> > * 雖然可以很快得到損失值，但沒有用Vectorizqtion來提高效率，整體的訓練效率低。
> 
> * 3. **Mini-batch gradient descent**: 是針對部分樣本。
> 
> > * 將全部樣本數，進行均分，例如切割5000份，10000/份。
> > * 每次得到損失值的時間可以接受，也有用到Vectorizqtion來提高效率。

<h3 id="2.2.1">2.2.1  圖解mini-batch gradient descent?</h3>

> ![100](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/100.png)
>

<h3 id="2.2.2">2.2.2  程式碼mini-batch gradient descent?</h3>

> ![101](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/101.png)
>

<h3 id="2.2.3">2.2.3  如何選擇適當mini-batch size?</h3>

> * 通常樣本數大於2000以上，就可以用mini-batch。
> * 大小的選擇通常是的2的次方，64,128,256,512...。

<h3 id="2.3.1">2.3.1  為什麼要了解Exponentially weighted averages?</h3>

> * 因為這是其他優於gradent decent算法的核心基礎概念。

<h3 id="2.3.2">2.3.2  什麼是Exponentially weighted averages?</h3>

> **例子: 倫敦的10日期溫均線**
> 
> ![102](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/102.png)
>
> **公式**
> 
> ![103](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/103.png)
>

<h3 id="2.4.1">2.4.1 exponentially是怎麼來的?</h3>

> ![104](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/104.png)
>
> * 不斷地將公式逐層帶入後，將最後的式子的每個項目拆成兩部分**theta,exponential**。

<h3 id="2.4.2">2.4.2 exponentially weighted averages的優勢?</h3>

> * 程式碼簡潔。
> * 計算速度快。
> * 佔用的memory少。

<h3 id="2.5.1">2.5.1 為什麼需要了解Bias correction?</h3>

> * 解決Exponentially weighted averages不準確的地方。

<h3 id="2.5.2">2.5.2  exponentially weighted averages存在的不準確在哪裡?</h3>
<h3 id="2.5.3">2.5.3  Bias correction是如何改善不準確?</h3>

> ![105](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/105.png)
>
> * **Bias correction公式**，讓初期的誤差不會那麼誇張，降低初期的誤差。
> * Bias correction並不是必須的，因為通常不會在乎早期訓練的誤差。 


<h3 id="2.6.1">2.6.1  momentum的概念</h3>

> ![106](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/106.png)
>
> * **概念: momentum = Gradient Descent + exponentially weighted averages**
> * 解決了mini-batch gradient descent在橫向跟縱向之間的矛盾。
> 
> > * 如果learning rate大，橫向雖然可以快速移動，在縱向的波幅會偏大，有可能會無法收斂。
> > 
> > * 如果learning rate小，在縱向的波幅會較小，但在橫向移動慢，收斂的速度慢。
> > 
> 
> * **解決方式:在每一個min batch裡，針對w,b進行加權平均，然後才進行更新**


<h3 id="2.6.2">2.6.2  momentum的實作</h3>

> ![107](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/107.png)
>


<h3 id="2.7.1">2.7.1  RMSprop的概念</h3>

> ![108](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/108.png)
>
> * **RMSprop = Root Mean Square prop**
> * 跟momentum相似，利用exponentially weighted averages來解決mini-batch gradient descent在橫向跟縱向之間的矛盾。
> * 跟momentum相異，利用exponentially weighted averages的平方根來進行更新。

<h3 id="2.8.1">2.8.1  Adam的概念</h3>

> ![109](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/109.png)
>
> * **Adam = Adaptive moment estimitation**
> * 優勢: 此優化算法可以被廣泛應用在不同情境下的模型。
> * 結合了momentum跟RMSprop。

<h3 id="2.9.1">2.9.1  Learning rate decay的概念</h3>

> ![110](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/110.png)
>
> * 透過降低learning rate來進行加速學習。
> * 在min-batch下，如果是固定learning rate。
> 
> > * 若設定大，學習快，但有可能很難達到最佳值。
> > * 若設定小，學習慢，但較容易達到最佳值。
> 
> * **learning rate 動態變化，一開始大，後來小。**
> 
> ![111](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/111.png)
>

<h3 id="2.10.1">2.10.1  瞭解local optimal跟saddle point</h3>

> ![112](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/112.png)
>
> * local optimal是J跟W形成一個碗型，所以在多個w下，就形成多個碗型，模型很容易到了某個碗型的最低點就停止訓練了，沒有辦法到最底層的碗的底部。
> * local optimal是淺層模型比較容易發生，但在深層模型就很難遇到，深層會遇到的是saddle point。
> * saddle point所帶來的問題，不是slop=0，停止訓練，而是有些地方訓練速度會變慢，因此上面講的那些優化算法，就是來加速這樣的狀況。
> 
> ![113](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/113.png)
>