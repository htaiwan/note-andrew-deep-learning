## Week 1. Practical aspects of Deep Learning

### 1.1 Train / Dev / Test sets
> [1.1.1  瞭解機器學習的本質為何](#1.1.1)
> 
> [1.1.2  如何分配Train / Dev / Test sets](#1.1.2)
> 
> [1.1.3  瞭解Train / Dev / Test的資料分布跟誤差](#1.1.3)

### 1.2 Bias / Variance
> [1.2.1  瞭解bias跟variance的定義跟彼此間關係](#1.2.1)
> 
> [1.2.2  什麼是bayes error](#1.2.2)

### 1.3 Basic Recipe for Machine Learning
> [1.3.1  ML的起手式](#1.3.1)

### 1.4 Regularization
> [1.4.1  regularization的意義](#1.4.1)
> 
> [1.4.2  理解L1, L2 regularization在Logistic regression的用法](#1.4.2)
> 
> [1.4.3  理解L1, L2 regularization在神經網路的用法](#1.4.3)

### 1.5 Why regularization reduces overfitting?
> [1.5.1  理解regularization是如何降低overfitting - 1](#1.5.1)
> 
> [1.5.2  理解regularization是如何降低overfitting - 2](#1.5.2)

### 1.6 Dropout Regularization
> [1.6.1  Dropout的概念](#1.6.1)
>
> [1.6.2  Dropout的實作](#1.6.2)

### 1.7 Understanding Dropout
> [1.7.1  瞭解Dropout是如何降低overfitting](#1.7.1)

### 1.8 Other regularization methods
> [1.8.1  瞭解其他regularization的方法](#1.8.1)

### 1.9 Normalizing inputs
> [1.9.1  如何對input進行標準化](#1.9.1)
> 
> [1.9.2  為什麼對input進行標準化，可以加速模型訓練](#1.9.2)

### 1.10 Vanishing / Exploding gradients
> [1.10.1  梯度爆炸和消失的原因](#1.10.1)

### 1.11 Weight Initialization for Deep Networks
> [1.11.1  如何將weight初始化成合適的大小](#1.11.1)

### 1.12 Numerical approximation of gradients

### 1.13 Gradient checking
> [1.13.1  Gradient checking的目的](#1.13.1)
> 
> [1.13.2  Gradient checking的實作](#1.13.2)

### 1.14 Gradient Checking Implementation Notes 
> [1.14.1  Gradient checking的實作注意事項](#1.14.1)

---

## Answers: 

<h3 id="1.1.1">1.1.1  瞭解機器學習的本質為何</h3>

> ![81](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/81.png)
>
> * 機器學習的本質就是將想法(idea)->執行(code)->實驗(experiement)，然後再校正想法，不斷地重複循環。
> * 循環實驗目的就是找出最佳化的超參數。
> * 在某個模型下的最佳化的超參數通常是無法直接複製到另一個模型下直接使用，是需要重新訓練的。

<h3 id="1.1.2">1.1.2  如何分配Train / Dev / Test sets</h3>

> ![82](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/82.png)
> 
> * 小數據(100): 0.6 / 0.2 / 0.2
> * 大數據(1000000): 0.99 / 0.005 / 0.005 

<h3 id="1.1.3">1.1.3  瞭解Train / Dev / Test的資料分布跟誤差</h3>

> ![83](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/83.png)
> 
> * Train / (Dev, Test): 這兩組數據在現實上是存在一定的差異，因為訓練是需要大量的數據，我們通常會使用可以便宜找到的大量數據，例如訓練時我們可能會從網路上下載貓的高清晰度圖片，這個跟實際操作時從usr那邊拿到的貓的圖片，可能不同。
> * Dev / Test: 這兩組的數據不應該存在分布的差異，主要都是應該拿現實真的會用到的資料，例如真的從使用者上獲取的貓的圖片。
> * 測試集不是必須的。 

<h3 id="1.2.1">1.2.1  瞭解bias跟variance的定義跟彼此間關係</h3>

> ![84](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/84.png)
> 
> * bias vs variance: 就是在資料的主要規律特徵跟噪音中做取捨。
> * high bias = under fitting = 沒有抓住資料的主要規律特徵。
> * high variance = over fitting = 太糾結數據中噪音，而模糊了資料的主要規律特徵。

<h3 id="1.2.2">1.2.2  什麼是bayes error</h3>

> ![85](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/85.png)
> 
> * bayes error: 可以將人類的識別誤差標準作為基準，可以是0，也有可能是15%。
> * (1): 若bayes error = 0 %
> * (2): overfitting = low bias = 訓練表現良好 = high variance = 訓練驗證表現差距大。
> * (3): underfitting = high bias = 訓練表現差 = low variance = 訓練驗證表現差距小。
> * (4): underfitting = high bias = 訓練表現差 = high variance = 訓練驗證表現差距大。
> * (5): fitting right = low bias = 訓練表現良好 = low variance = 訓練驗證表現差距小。

<h3 id="1.3.1">1.3.1  ML的起手式</h3>

> ![86](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/86.png)
> 
> * 通過不斷的循環圖中的步驟，降低high bias跟high variance，進而優化模型。 
> * 1. 先降低bias:
> 
> > * 更大更深的模型。
> > * 不同的模型設計。
> 
> * 2. 再來降低variance:
> 
> > * 更多的數據。
> > * 正規化處理。
> > * 不同的模型設計。
>
> * **傳統的機器學習，很難做到bias跟variance同時降低，通常都是trade off，這也是深度學習的優勢所在**

<h3 id="1.4.1">1.4.1  regularization的意義</h3>

> * 用來解決overfitting, high variance的問題，也就是驗證結果跟訓練結果差異很大。
> * **regularization阻止模型學習噪音數據，強化主要的規律特徵數據，弱化噪音數據**

<h3 id="1.4.2">1.4.2  理解L1, L2 regularization在Logistic regression的用法</h3>

> ![87](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/87.png)
> 
> * L1,L2透過lambd來控制w的大小，進而控制模型對噪音數據的學習。
> * (1): 原本的cost函數
> * (2): L1 regularization
> * (3): L2 regularization
> * 當lamba很大時，因為要cost函數要最小化，所以w就必須很小，進而弱化噪音數據。


<h3 id="1.4.3">1.4.3  理解L1, L2 regularization在神經網路的用法</h3>

> ![88](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/88.png)
> 
> * 在反向傳遞時，weight decay利用lambd，來控制w的大小。

<h3 id="1.5.1">1.5.1  理解regularization是如何降低overfitting - 1</h3>

> ![89](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/89.png)
> 
> * **通過壓縮模型大小，來控制噪音數據的學習。**

<h3 id="1.5.2">1.5.2  理解regularization是如何降低overfitting - 2</h3>

> ![90](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/90.png)
> 
> * **將非線形模型轉成線型模型，弱化模型的學習能力。**

<h3 id="1.6.1">1.6.1  Dropout的概念</h3>

> * 透過隨機拋棄神經元，來壓縮模型大小。

<h3 id="1.6.2">1.6.2  Dropout的實作</h3>

> ![91](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/91.png)
> 
> * (1): d3(matrix), 隨機生成80%的1, 20%的0。
> * (2): a3跟d3相乘，表示隨機保留80%的神經元，20%的神經元被拋棄。
> * (3): a3除以keep-prob，表示在拋棄20%神經元後，神經網路的效力應該會下降，為了讓神經網路的效力保持不變。

<h3 id="1.7.1">1.7.1  瞭解Dropout是如何降低overfitting</h3>

> * **隨機丟棄神經元，縮小模型，降低噪音數據的學習**
> * **隨機選取神經元，讓每個特徵的預測貢獻度更佳平均，不會拘泥在某個噪音特徵上**
 
<h3 id="1.8.1">1.8.1  瞭解其他regularization的方法</h3>

> * 1. **data augumention(數據變形)**，來擴大原有的數據量，來降低variance。
> 
> > * 例如貓的圖片，如果所有貓的圖片都是貓身體在左邊，那模型可能就會執著於“身體在左邊”就是貓的重要特徵，因此透過圖片的反轉，產生左右相反的圖片，讓模型會執著於這樣的特徵。
> 
> * 2. **early stop。**
> 
> > * 當訓練到一半時，當發現dev error出現最優值，就停止訓練。用提前結束訓練，找到overfitting跟underfitting之前的一個平衡點。
> > * 缺點就是有可能無法讓模型達到最優化。

<h3 id="1.9.1">1.9.1  如何對input進行標準化</h3>

> ![92](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/92.png)
>
> * 可以加速模型的訓練。
> * 平移 ＋ 標準差的縮放。
> * traing/dev/test的數據都要一起進行標準化。

<h3 id="1.9.2">1.9.2  為什麼對input進行標準化，可以加速模型訓練</h3>

> ![93](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/93.png)
>
> * input未標準化 -> 導致learning rate必須設置很小 -> 不然容易divergence(y軸飄移很大) -> 也導致gradient decent要花更久的時間才能走到最小損失值。
> * input標準化 -> learning rate可以設置較大 -> 不容易divergence -> 也導致gradient decent可以比較快的走到最小損失值。

<h3 id="1.10.1">1.10.1  梯度爆炸和消失的原因</h3>

> ![94](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/94.png)
>
> * 在很深的模型中，每一層的weight，經過正向或反向傳播時，會有**指數效應**
> * **如果weight中有大於1的數，產出預測值時，就會出現梯度爆炸**
> * **如果weight中有小於1的數，產出預測值時，就會出現梯度消失**
> * 解決方式: **就是要將weight初始化成合適的大小**


<h3 id="1.11.1">1.11.1  如何將weight初始化成合適的大小</h3>

> ![95](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/95.png)
>
> * 盡量將weight控制在1的附近，讓每一層的weight都不要比1大太多，也不要比1小太多。
> * (1): 針對relu的weight的初始化公式。
> * (2): 針對tanh的weight的初始化公式。

<h3 id="1.13.1">1.13.1  Gradient checking的目的</h3>

> ![96](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/96.png)
>
> * 驗證反向傳遞是否正常運作。
> * 精準度高，但計算量大。

<h3 id="1.13.2">1.13.2  Gradient checking的實作</h3>

> ![97](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/97.png)
>
> ![98](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/98.png)
>
> * 1. 將所有的參數w,b都放到一個很大的vector1。
> * 2. 將實際得到的參數的gradient也都放到另外一個很大的vector2。
> * 3. 目標: 檢查vector2是否真的是vector1的導數。

<h3 id="1.14.1">1.14.1  Gradient checking的實作注意事項</h3>

> ![99](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/99.png)
>
> * 1. 訓練時，不要使用gradient checking，因為計算費時。
> * 2. 如果gradient checking，發現backprop的確有誤時，要在w,b尋找問題在哪裡。
> * 3. 如果有使用regularation時，也要同時更新Ｊ的公式，加上regularation的項目。
> * 4. 對於dropout模型，先關閉dropout，跑模型，在用gradient checking檢查backprop。
> * 5. 先訓練幾次後，再開始做gradient checking。

