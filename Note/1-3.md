## Week 3. Shallow neural networks

### 3.1 Neural Networks Overview?
> [3.1.1 什麼是Neural Networks](#3.1.1)

### 3.2 Neural Network Representation?
> [3.2.1 了解多個神經元的matrix運算(單個樣本)](#3.2.1)

### 3.3 Vectorizing across multiple examples?
> [3.3.1 多個樣本的運算](#3.3.1)

### 3.4 Activation functions?
> [3.4.1 瞭解多個Activation functions的比較](#3.4.1)

### 3.5 Why do you need non-linear activation functions?
> [3.5.1 為什麼Activation functions都是非線性的函數?](#3.5.1)

### 3.6 Derivatives of activation functions
> [3.6.1 各個activation functions的導數為何?](#3.6.1)

### 3.7 Gradient descent for Neural Networks
> [3.7.1 知道正向跟反向傳播的公式為何?](#3.7.1)

---

## Answers: 

<h3 id="3.1.1">3.1.1 什麼是Neural Networks</h3>

> ![67](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/67.png)
> 
> - 簡單(上):只有輸入層(3個neurons)，和輸出層(1個neurons)。
> - 簡單(下):輸入層(3個neurons)，隱藏層(3個neurons)，和輸出層(1個neurons)。

<h3 id="3.2.1">3.2.1 了解多個神經元的matrix運算(單個樣本)</h3>

> ![68](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/68.png)

<h3 id="3.3.1">3.3.1 多個樣本的運算</h3>

> ![69](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/69.png)
> 
> - 向量化，處理多個樣本(不用使用for loop m個樣本)。

<h3 id="3.4.1">3.4.1 瞭解多個Activation functions的比較</h3>

> ![70](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/70.png)
> 
> - 依layer type區分:
> 
> > - input layer: 不需要linear combination或activation function。
> > - hidden layer: tanh會優於sigmoid。
> > - output layer: 對於2分類的問題，會選擇sigmoid。
> 
> - tanh和sigmoid的缺點: 當z比較大時，例如>2或<-2，此時這兩個activation function的slope都會接近0，導致梯度消失，也就是會讓淺層的參數更新的速度非常慢。
> 
> - relu取代tanh: 因為多數情況z都會大於0，slope都會是1，不會像tanh出現接近0的情況。
> - 實作上，在不同情況下，不同Activation functions可能會有不同的效果，最好還是都稍微試試看，比較一下，看哪個真的比較好。

<h3 id="3.5.1">3.5.1 為什麼Activation functions都是非線性的函數?</h3>

> - 因為真實世界的數據是複雜的，不會只是單單只靠線性組合就可以模擬出真實的結果。
> - 當神經網路，移除掉非線性的Activation functions時，不管神經網路再多層，最終還是個線性組合，無法滿足真實世界的實際需求。
> - 非線性的Activation functions就像是神經網路中的一個催化劑，讓各個神經元之間產生化學變化來模擬出真實世界的需求。

<h3 id="3.6.1">3.6.1 各個activation functions的導數為何?</h3>

> ![71](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/71.png)
> 
> ![72](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/72.png)
> 
> ![73](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/73.png)
> 

<h3 id="3.7.1">3.7.1 知道正向跟反向傳播的公式為何?</h3>

> ![74](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/74.png)
>
> - 推導過一次，了解就好，因為每隔一陣子就會忘記怎麼推，重點知道公式長怎樣就好了。
