
## Week 3. Object detection

### Detection algorithms

### 3.1 Object Localization

> [3.1.1 localization跟detection的差異?](#3.1.1)
> 
> [3.1.2 如何用單個模型來實現分類加上定位?](#3.1.2)

### 3.2 Landmark Detection

> [3.2.1 什麼是Landmark Detection?](#3.2.1)

### 3.3 Object Detection

> [3.3.1 瞭解車輛偵測模型例子?](#3.3.1)
> 
> [3.3.2 sliding window的功用?](#3.3.2)

### 3.4 Convolutional Implementation of Sliding Windows

> [3.4.1 如何將FC轉成Conv?](#3.4.1)
> 
> [3.4.2 理解conv-implementation的加速計算的設計?](#3.4.2)

### 3.5 Bounding Box Predictions

> [3.5.1 理解sliding window在定位上面對的難題?](#3.5.1)
> 
> [3.5.2 理解YOLO 算法?](#3.5.2)

### 3.6 Intersection Over Union

> [3.6.1 IOU算法的功用?](#3.6.1)

### 3.7 Non-max Suppression

> [3.7.1 理解non-max suppression算法?](#3.7.1)
> 
> [3.7.2 non-max suppression的計算步骤?](#3.7.2)

### 3.8 Anchor Boxes

> [3.8.1 理解anchor box是如何解决一個grid cell里有2個中心點的問題？](#3.8.1)
> 
> [3.8.2 如何避免预测的anchor boxes與對應的物體被錯誤配對？](#3.8.2)

### 3.9 YOLO Algorithm

> [3.9.1 理解YOLO模型的構建流程？](#3.9.1)

### 3.10 (Optional) Region Proposals

---

## Answers:

<h3 id="3.1.1">3.1.1 localization跟detection的差異?</h3>

> * classification: 識別圖片中的一個物品類別。
> * classification + localization: 識別 + 定位(方框)。 <- 單個物品
> * detection: 識別 + 定位(方框)。 <- 多個物品(多種類)

<h3 id="3.1.2">3.1.2 如何用單個模型來實現分類加上定位?</h3>

> ![27](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/27.png)
> 
> Target Y: 
> > * pc: background, object (先決條件，節省計算)。
> > * bx, by, bw, bh: box location。
> > * c1, c2, c3: 0 or 1 (種類)。

<h3 id="3.2.1">3.2.1 什麼是Landmark Detection?</h3>

> * 更細緻的localization需求，例如識別人臉，肢體動作，單純只靠bx, by, bw, bh是不夠，需要更多的關鍵點進行識別。

<h3 id="3.3.1">3.3.1 瞭解車輛偵測模型例子?</h3>

> * Car detection
> 
> > * 1. 先建立 car classification model。
> > * 2. 再進行 Sliding Window detection。

<h3 id="3.3.2">3.3.2 sliding window的功用?</h3>

> * sliding window = filiter 
> * 每次擷取sliding window所框住的圖片丟入model去判別有車還是無車。
> * 設計不同sliding window，stride size，可以確保框住車子。
> * sliding window，stride size 小
> 
> > * 一個樣本，掃描次數太多，計算成本高，速度慢。 
> 
> * sliding window，stride size 大
> 
> > * 精準度下降。 

<h3 id="3.4.1">如何將FC轉成Conv?</h3>

> ![28](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/28.png)
> 
> * 第一層FC -> Conv
> 
> > * 替換成400個5x5x16的filter
> 
> * 第二層FC -> Conv
> 
> > * 替換成400個1x1x400的filter
> 
> * 第三層FC -> Con
> 
> > * 替換成4個1x1x400的filter(因為這裡的output需求是4分類)

<h3 id="3.4.2">3.4.2 理解conv-implementation的加速計算的設計?</h3>

> ![29](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/29.png)
> 
> * 首先，建立一個變形好的CNN model(最後三層的FC轉成conv)
> * 原本做法: 將測試數據(16x16x3)，會分成4張14x14x3的圖片樣本，通過模型，計算4次，分別判斷每張圖是否有物品存在，計算成本高，效率低。
> * 改良做法: 發現原本做法中的4次計算有大量重複的內容，因此直接將原本測試數據(16x16x3)直接丟入變形好的model中，不用分割成4張圖。
> 
> > * 一次計算，完成原本4次的計算量的結果，大大提升原本的計算效率。
> > * 不同的是layer output size變大。(1x1x4 -> 2x2x4) 
> > * 其實就是原本做法的4張截圖的結果合併。 

<h3 id="3.5.1">3.5.1 理解sliding window在定位上面對的難題?</h3>

> * sliding window的本質，是有規則性的原圖進行截圖，但這些規則性的截圖有可能無法精準框住原圖上的目標物。

<h3 id="3.5.2">3.5.2 理解YOLO 算法?</h3>

> ![30](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/30.png)
>
> * You Only Look Once (YOLO)
> * 能提供更精準的方框。
> 
> > * 這裡簡化成 3x3 grid cell，一般實作上是分割成 19x19，兩個物品出現在同一個cell的機率小很多。
> > * 分別將這個9張圖，先進行人工標示，若圖中有識別物，則需要補上識別物的中心點座標跟長寬等相關資訊。然後利用這些測試數據對模型進行訓練，最後訓練出來的模型，就可以判斷出是否有物品，什麼類別，中心點跟長寬。
> > * output tensor size: 3x3x8(8是表示y vector的大小) ，一次計算完成9 cell的結果，這是因為convolutional implementation的加速計算，提升運算效率。

<h3 id="3.6.1">3.6.1 IOU算法的功用?</h3>

> ![31](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/31.png)
>
> * Intersection Over Union (IOU)
> * 衡量object detetection的成效。
> 
> > * 模型計算出來的bounding box跟實際target的bounding box的差距有多遠。
> > * IOU值越高，表示成效越好。

<h3 id="3.7.1">3.7.1 理解non-max suppression算法?</h3>

> ![32](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/32.png)
>
> * 如何決定唯一的bounding box
> 
> > * 問題點: 在19x19的grid中，理論上車輛的中心點應該只會在一個grid cell中，但實際上多個相近的grid cell都會宣稱自己抓住了中心點，也就是會有多個bounding boxes。
> > * 從bounding box的pc值來決定，選擇pc值最高的。

<h3 id="3.7.2">3.7.2 non-max suppression的計算步骤?</h3>

> ![33](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/33.png)
>
> * 具體作法
> 
> > * 先丟棄prob <= 0.6的bounding box。
> > * 開始進行while loop。(假設有多個 > 0.6的bounding box)
> > * 選擇剩餘bounding box中pc值最高的。
> > * 將剩餘bounding box與選中的bounding box的IOU >= 0.5的那些box一一丟棄。
> > * 直到所有物品都只有唯一bounding box。

<h3 id="3.8.2">3.8.1 理解anchor box是如何解决一個grid cell里有2個中心點的問題？</h3>

> ![34](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/34.png)
>
> * anchor box是用來解決overlapping object。
> 
> > * 問題點: 圖中的車與人的中心點重合，2個中心點皆在底層中間的grid cell中，先前的target y中只能處理一個grid cell中一個類別物品的bounding box，但現在要如何在一個grid cell中處理兩個物品的bounding box。
> > * 解決方式: 使用anchor box，針對不同物品設計相對應的anchor box，然後改變target y，疊加兩種不同類別物品anchor box，讓y可以描述兩個不同類型的anchor box。

<h3 id="3.8.2">3.8.2 如何避免预测的anchor boxes與對應的物體被錯誤配對？</h3>

> * 問題點:1個grid cell中有兩個物體，2種anchor box，要如何避免模型預測出來橫向的bounding box給到了擁有縱向anchor box的物體。
> * 解決方式:計算bouding box和anchor box的IOU，選擇IOU值更大，重合度高的。

<h3 id="3.9.1">3.9.1 理解YOLO模型的構建流程？</h3>

> ![35](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/35.png)
>
> * Training:
> 
> > * input: 100x100x3
> > * 將input切割成9宮格，一次丟入ConNet
> > * output: 3x3x16 (3x3 9宮格的圖) (16=2x8=anchor數x資訊數(pc,x,y,w,h,c1,c2,c3)) 
> >
>  
> ![36](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/36.png)
>
> * Predition:
> 
> > * 先丟棄機率 < 0.6的cell。
> > * 剩餘的cell，若有存在多個的bounding box，則使用non-max suppression，來決定每個物品的唯一的bounding box。
