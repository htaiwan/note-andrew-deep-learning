## Week 2. Neural Networks Basics

### 2.1 Binary Classification
> [2.1.1 什麼是Binary Classification](#2.1.1)
> 

### 2.2 Logistic Regression
> [2.2.1 什麼是Logistic Regression](#2.2.1)
> 

### 2.3 Logistic Regression Cost Function
> [2.3.1 什麼是Logistic Regression Cost Function](#2.3.1)
> 

### 2.4 Gradient Descent
> [2.4.1 什麼是Gradient Descent](#2.4.1)
> 

### 2.5 Derivatives
> [2.5.1 什麼是Derivatives](#2.5.1)
> 

### 2.6 More Derivative Examples

### 2.7 Computation graph
> [2.7.1 什麼是Computation graph](#2.7.1)
> 

### 2.8 Derivatives with a Computation Graph
> [2.8.1 什麼是Derivatives with a Computation Graph](#2.8.1)
> 

### 2.9 Logistic Regression Gradient Descent
> [2.9.1 什麼是ogistic Regression Gradient Descent](#2.9.1)
> 

### 2.10 Gradient Descent on m Examples
> [2.10.1 什麼是Gradient Descent on m Examples](#2.10.1)
> 

---

## Answers: 

<h3 id="2.1.1">2.1.1 什麼是Binary Classification</h3>

> ![59](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/59.png)
> 
> - input: 圖片(64x64x3)。
> - ouput: 1(是貓),0(不是貓) 

<h3 id="2.2.1">2.2.1 什麼是Logistic Regression</h3>

> ![60](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/60.png)
> 
> - 用logsitc regression來解釋Binary Classification，表示**當給予x，y=1的機率是多少**。
> - Input: x (數據)
> - Parameter: w, b
> - Output: **是將w,b,x經過線性組合後通過一個sigmoid函數，得到一個0~1的輸出值**

<h3 id="2.3.1">2.3.1 什麼是Logistic Regression Cost Function</h3>

> ![61](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/61.png)
> 
> - 數學式
>
> > - 線性組合:(1)
> > - sigmoid function:(2)
> > - loss function:(4)
> > - cost function:(5)
>
> - Logistic Regression Cost Function: **是求Logistic Regression function的輸出值跟實際值的差值得總和。**
> - 直觀的loss function應該要像(3)，但是3並不利於之後的Gradient Descent的計算，因為loss function的圖像(6)。
> - (4)雖然不直觀，但的確有滿足y=0,y=1時，我們對輸出值的要求，也利於之後的Gradient Descent的計算。


<h3 id="2.4.1">2.4.1 什麼是Gradient Descent</h3>

> ![62](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/62.png)
> 
> - 找出一組最佳的W，使得J(W,b)為最小，也就是讓Cost Function的值為最小。
> - w更新的方向跟大小是由(1),(2)乘積決定，(1)決定每次變化的步伐大小。

<h3 id="2.5.1">2.5.1 什麼是Derivatives</h3>

> - 導數(斜率)其實就是函數在某個微小的區間內的輸出值的變化。

<h3 id="2.7.1">2.7.1 什麼是Computation graph</h3>

> ![63](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/63.png)
> 
> - 拆分每一個計算步驟，方便正向跟反向的計算。
> - 正向計算: 拆分3(a+bc)的計算步驟，一步一步的求出J(loss or cost function)。
> - 反向計算: 將誤差值反推更新a,b,c，實現不斷更新cost，讓cost最小化的目的。

<h3 id="2.8.1">2.8.1 什麼是Derivatives with a Computation Graph</h3>

> ![64](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/64.png)
> 
> - 反向計算的拆分過程，利用**chain rule**來連接。
> - 先求(1)，再來要求(2)，我們必須透過chain rule的方式(3)，因為我們可以算出(1),(4)，所以可以得到(2)。

<h3 id="2.9.1">2.9.1 什麼是ogistic Regression Gradient Descent</h3>

> ![65](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/65.png)
> 
> - 反向計算的拆分過程，利用**chain rule**來連接。
> - 先求(1)，再來要求(2)，我們必須透過chain rule的方式(3)，有了(2)，再利用一次chain rule可以得到(4),(5),(6)。

<h3 id="2.10.1">2.10.1 什麼是Gradient Descent on m Examples</h3>

> ![66](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/66.png)
> 
> - 需要2個for loop, 第一個loop是針對每個樣本，第二個loop是針對每個樣本中的每個feature的誤差進行累積。
> - 將總誤差平均分攤給m個樣本
> - 更新權重
> - for loop計算效率差，要透過vectorization來代替for loop。
