## Week 3. Hyperparameter tuning, Batch Normalization and Programming Frameworks

### 3.1 Tuning process

> [3.1.1  超參數處理的priority](#3.1.1)
> 
> [3.1.2  超參數的調整準則](#3.1.2)

### 3.2 Using an appropriate scale to pick hyperparameters

> [3.2.1  如何選擇合適的scale來調整超參數](#3.2.1)

### 3.3 Hyperparameters tuning in practice: Pandas vs. Caviar

> [3.3.1  超參數的實戰訓練方式](#3.3.1)

### 3.4 Normalizing activations in a network

> [3.4.1  為什麼要 Normalizing activations？](#3.4.1)
> 
> [3.4.2  Batch Norm的實作](#3.4.2)

### 3.5 Fitting Batch Norm into a neural network

> [3.5.1  如何將batch norm實際應用在neural network?](#3.5.1)
> 
> [3.5.2  batch norm + mini-batches](#3.5.2)
> 
> [3.5.3  程式碼邏輯](#3.5.3)

### 3.6 Why does Batch Norm work?

> [3.6.1  什麼是convariance shift?](#3.6.1)
> 
> [3.6.2  Batch Norm是如何解決convariance shift?](#3.6.2)

### 3.7 Batch Norm at test time

> [3.7.1  如何在test的時候，進行Batch Norm?](#3.7.1)

### 3.8 Softmax Regression

> [3.8.1  什麼是Softmax Regression?](#3.8.1)

### 3.9 Training a softmax classifier

> [3.9.1  進一步了解Softmax](#3.9.1)
> 
> [3.9.2  Softmax的loss function](#3.9.2)
> 
> [3.9.3  Softmax的back prop](#3.9.3)

### 3.10 Deep learning frameworks

> [3.10.1  選擇一個好的Deep learning frameworks的準則](#3.10.1)

### 3.11 TensorFlow

> [3.11.1  TensorFlow簡易的tutorial](#3.11.1)

---

## Answers: 

<h3 id="3.1.1">3.1.1 超參數處理的priority</h3>

> ![113-1](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/113-1.png)
>
> * 1. learning rate
> * 2. momentum Beta, hidden unit, min-batch size
> * 3. layers, learning rate decay


<h3 id="3.1.2">3.1.2 超參數的調整準則</h3>

> ![114](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/114.png)
>
> ![115](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/115.png)
>
> * 1. 隨機選取超參數的組合。
> * 2. 聚焦在更小區域，在隨機選取組合。

<h3 id="3.2.1">3.2.1 如何選擇合適的scale來調整超參數</h3>

> * 1. **隨機:**
>
> ![116](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/116.png)
>
> * 2. **log scale:**
>
> ![117](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/117.png)
> 
> * 3. **權重平均:**
>
> ![118](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/118.png)

<h3 id="3.3.1">3.3.1 超參數的實戰訓練方式</h3>

> ![119](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/119.png)
> 
> * 熊貓 vs 魚卵

<h3 id="3.4.1">3.4.1 為什麼要 Normalizing activations？</h3>

> ![120](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/120.png)
> 
> **主要目的就是要加速訓練。**

<h3 id="3.4.2">3.4.2 Batch Norm的實作</h3>

> ![121](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/121.png)

<h3 id="3.5.1">3.5.1 如何將batch norm實際應用在neural network?</h3>

> ![122](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/122.png)

<h3 id="3.5.2">3.5.2 batch norm + mini-batches</h3>

> ![123](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/123.png)

<h3 id="3.5.3">3.5.3 程式碼邏輯</h3>

> ![124](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/124.png)

<h3 id="3.6.1">3.6.1 什麼是convariance shift?</h3>

> ![125](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/125.png)
> 
> * 因為數據分佈不同，導致模型預測結果的誤差。

<h3 id="3.6.2">3.6.2 Batch Norm是如何解決convariance shift?</h3>

> ![126](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/126.png)

<h3 id="3.7.1">3.7.1 如何在test的時候，進行Batch Norm?</h3>

> ![127](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/127.png)
> 
> * 在訓練過程中，收集所有層在不同mini-batch所得到的平均值跟方差，最後在做加權平均，然後直接將這個兩個值用在測試中。

<h3 id="3.8.1">3.8.1 什麼是Softmax Regression?</h3>

> ![128](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/128.png)
> 
> * 用於2個以上的輸出值。

<h3 id="3.9.1">3.9.1 進一步了解Softmax?</h3>

> ![129](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/129.png)
> 
> * Logistic Regression: 2元分類 
> * Softmax Regression: 多元分類 (soft: 表示每個分類都是有個預測值)

<h3 id="3.9.2">3.9.2 Softmax的loss function</h3>

> ![130](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/130.png)
> 

<h3 id="3.9.3">3.9.3 Softmax的back prop</h3>

> ![131](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/131.png)
> 

<h3 id="3.10.1">3.10.1  選擇一個好的Deep learning frameworks的準則</h3>

> ![132](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/132.png)
> 
