
## Week 2. Deep convolutional models: case studies

### Case studies

### 2.1 Why look at case studies?

> [2.1.1 為什麼要學習一些經典模型結構?](#2.1.1)

### 2.2 Classic Networks 

> [2.2.1 LeNet-5結構?](#2.2.1)
> 
> [2.2.2 AlexNet結構?](#2.2.2)
> 
> [2.2.3 VGG結構?](#2.2.3)

### 2.3 ResNets

> [2.3.1 為什麼深度模型結構很難訓練?](#2.3.1)
> 
> [2.3.2 什麼是residual block?](#2.3.2)
> 
> [2.3.3 理解plain network與ResNet的效果對比?](#2.3.3)

### 2.4 Why ResNets Work

> [2.4.1 ResNet的skip connection是如何解決梯度消失和爆炸的问题?](#2.4.1)

### 2.5 Networks in Networks and 1x1 Convolutions

> [2.5.1 理解network in network的結構?](#2.5.1)

### 2.6 Inception Network Motivation

> [2.6.1 理解inception的核心?](#2.6.1)
> 
> [2.6.2 理解inception的成本問題是如何解決?](#2.6.2)

### 2.7 Inception Network

> [2.7.1 理解一個完整的Inception module?](#2.7.1)
> 
> [2.7.2 如何理解中間預測層的功用?](#2.7.2)


### Practical advices for using ConvNets

### 2.8 Using Open-Source Implementation 

### 2.9 Transfer Learning 

> [2.9.1 為什麼需要Transfer Learning?](#2.9.1)
> 
> [2.9.2 如何讓transfer learning的訓練速度加快?](#2.9.2)
> 

### 2.10 Data Augmentation

> [2.10.1 常見的擴充樣本圖片數量的方法?](#2.10.1)
> 

### 2.11 State of Computer Vision 

> [2.11.1 模型的知識來源?](#2.11.1)

---

## Answers:

<h3 id="2.1.1">2.1.1 為什麼要學習一些經典模型結構?</h3>

> * 有些經典的結構設計，也許可以拿來其他的model中裡面使用。 

<h3 id="2.2.1">2.2.1 LeNet-5結構?</h3>

> ![11](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/11.png)
> 
> * no padding, 新合成的圖長寬逐漸縮小，channel逐漸變大，filter逐漸增多。
> * small model, 60K parameters。
> * conv -> conv -> fc -> fc -> output。
> * softmax layer with 10 output neurons。

<h3 id="2.2.2">2.2.2 AlexNet結構?</h3>

> ![12](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/12.png)
> 
> * 使用relu。
> * 分別在兩個 gpus 進行計算。
> * bigger model, 60M parameters。

<h3 id="2.2.3">2.2.3 VGG結構?</h3>

> ![13](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/13.png)
> 
> * 固定的conv filter & max pool filter。
> * 連續2或3次 conv layer。
> * filter個數每次都翻倍。
> * 138M parameters。

<h3 id="2.3.1">2.3.1 為什麼深度模型結構很難訓練?</h3>

> * 當深度越深，w,b可能會因為vanish gradient而接近0，因此繼續訓練下去，也很難更新到參數。

<h3 id="2.3.2">2.3.2 什麼是residual block?</h3>

> ![14](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/14.png)
> 
> * 2 layers + skip connection。
> * ResNet是基於residual block構建而成。
> * output a = 既有short cut也有main path所組合而成的。

<h3 id="2.3.3">2.2.3 理解plain network與ResNet的效果對比?</h3>

> ![15](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/15.png)
> 
> * plain network: 沒有short cut的network。
> * ResNet可以隨著深度越深，training error逐漸下降。

<h3 id="2.4.1">2.4.1 ResNet的skip connection是如何解決梯度消失和爆炸的问题?</h3>

> ![16](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/16.png)
> 
> * 同樣是深模型，w,b可能接近0，因為short cut作用，a[l+2]不是0而是a[l]，此時就可以有效反向傳遞進行參數更新。
> * 同時採用same padding，或者為a[l]添加weight matrix，強迫a[l+2]跟a[l]的shape保持一至，方便計算。

<h3 id="2.5.1">2.5.1 理解network in network的結構?</h3>

> ![17](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/17.png)
> 
> * network in network = 1x1 conv 。
> * 可以讓input跟ouput的長寬保持一至。
> * 可以讓output的channel跟input一致，或者變大變小。

<h3 id="2.6.1">2.6.1 理解inception的核心?</h3>

> ![18](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/18.png)
> 
> * 希望可以同時使用所有的filter跟pooling，將所有的結果進行疊加。

<h3 id="2.6.2">2.6.2 理解inception的成本問題是如何解決?</h3>

> ![19](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/19.png)
> 
> * output上的每個點(28x28x32)都是filter跟input掃描而來(5x5x192)。
> * 總共需要120M的點對點的乘法運算。
> 
> ![20](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/20.png)
> 
> * 透過1x1 conv降低成為原本的計算量的1/10。

<h3 id="2.7.1">2.7.1 理解一個完整的Inception module?</h3>

> ![21](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/21.png)
> 

<h3 id="2.7.2">2.7.2 如何理解中間預測層的功用?</h3>

> ![22](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/22.png)
> * 可以用regularization的效果，避免overfitting。

<h3 id="2.9.1">2.9.1 為什麼需要Transfer Learning?</h3>

> ![23](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/23.png)
> 
> * 拿別人現成訓練好的模型，用來直接訓練自己的數據。
> * 步驟 :
> 
> > 1. 先下載別人訓練好的模型跟weight。
> > 2. 根據自己的需求，替換後面的的output，保留前面層數跟weight。
> > 3. 一般來說，若自己的測試數據越多，越可以替換後面的層數多一點。

<h3 id="2.9.2">2.9.2 如何讓transfer learning的訓練速度加快?</h3>

> * 避免每一次epoch，都要讓X經過前面固定不變的layer跟weights。
> * 做法: 先計算過一次後，存在local端，之後直接讀取local的數據就可以。

<h3 id="2.10.1">2.10.1 常見的擴充樣本圖片數量的方法?</h3>

> ![24](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/24.png)
> 
> ![25](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/25.png)
> 
> * mirroring, random cropping, color shiftting。

<h3 id="2.11.1">2.11.1 模型的知識來源?</h3>

> ![26](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/26.png)
> 
> * 1. 數據(features and label)。
> * 2. 人工植入知識(hand-made-feature)。 <- 數據不夠機器學習時，只好靠工人智慧了。
