## Week 4. Deep Neural Networks

### 4.1 Deep L-layer neural network
> [4.1.1  瞭解Deep L-layer neural network的基本表示](#4.1.1)

### 4.2 Getting your matrix dimensions right 
> [4.2.1  瞭解w,b的shape](#4.2.1)

### 4.3 Why deep representations?
> [4.3.1  為甚麼需要深度的神經網路](#4.3.1)

### 4.4 Forward and Backward Propagation
> [4.4.1  瞭解Forward Propagation的在某一層的表示](#4.4.1)
> 
> [4.4.2  瞭解backward Propagation的在某一層的表示](#4.4.2)
> 
> [4.4.3  summary](#4.4.3)

### 4.5 Parameters vs Hyperparameters
> [4.5.1  參數跟超參數的區別](#4.5.1)
> 

### 4.6 What does this have to do with the brain?
> [4.6.1  神經網路跟腦科學關係](#4.6.1)
> 

---

## Answers: 

<h3 id="4.1.1">4.1.1 瞭解Deep L-layer neural network的基本表示</h3>

> ![75](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/75.png)


<h3 id="4.2.1">4.2.1 瞭解w,b的shape </h3>

> ![79](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/79.png)
> 
> * w,b的shape，要如何確保正確。

<h3 id="4.3.1">4.3.1 為甚麼需要深度的神經網路 </h3>

> * 可以獲取更複雜的特徵值。
> * 如果淺層也想獲取複雜的特徵值，必須要產生大量的神經元，進行相對於深度的神經網路更龐大的計算才能獲取。

<h3 id="4.4.1">4.4.1 瞭解Forward Propagation的在某一層的表示</h3>

> ![76](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/76.png)
> 
> * cache是用來方便之後backward的計算。

<h3 id="4.4.2">4.4.2 瞭解backward Propagation的在某一層的表示</h3>

> ![77](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/77.png)
> 
> * 注意output的公式。

<h3 id="4.4.3">4.4.3 summary </h3>

> ![78](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/78.png)
> 
> * 如何應用cache的資料，跟backward公式進行反向推導。

<h3 id="4.5.1">4.5.1 參數跟超參數的區別 </h3>

> ![80](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/80.png)
> 
> * 超參數主要是可以人為控制，也是我們在訓練優化NN時，主要就是在調整最佳的超參數。
> * 對於任何模型，隨著擁有數據量的改變，應該要定期調整超參數，來確保模型在最佳化的狀態下。

<h3 id="4.6.1">4.6.1 神經網路跟腦科學關係 </h3>

> * 神經網路只是嘗試在進行最佳化的mapping，跟腦科學的複雜性其實差異很大，所以拿神經網路來模擬人類的學習其實並不恰當。
