## Week 4. Special applications: Face recognition & Neural style transfer

### Face Recognition

### 4.1 What is face recognition?

> [4.1.1 理解face verification vs face recognition？](#4.1.1)
 
### 4.2 One Shot Learning

> [4.2.1 什麼是one shot learning problem？](#4.2.1)
> 
> [4.2.2 為什麼不能使用CNN？](#4.2.2)
> 
> [4.2.3 如何用similarity function 解決one shot learning problem？](#4.2.3)

### 4.3 Siamese Network

> [4.3.1 什麼是Siamese network？](#4.3.1)

### 4.4 Triplet Loss

> [4.4.1 理解triplet loss的核心部分？](#4.4.1)
> 
> [4.4.2 margin的必要性？](#4.4.2)
> 
> [4.4.3 為什麼要加入max函數設計？](#4.4.3)
> 
> [4.4.4 訓練時，一個identity需要有多個樣本？](#4.4.4)
> 
> [4.4.5 訓練時，為什麼不能隨機選擇圖片A，P，N？](#4.4.5)

### 4.5 Face Verification and Binary Classification

> [4.5.1 如何透過binary classification來實現Face verification / recognition？](#4.5.1)

---

### Neural style transfer

### 4.6 What is neural style transfer?

> [4.6.1 什麼是Neural style transfer？](#4.6.1)
>

### 4.7 What are deep ConvNets learning?

> [4.7.1 deep CNN在不同layer上學到什麼？](#4.7.1)
>
> [4.7.2 如何理解多個“nine image patches“？](#4.7.2)


### 4.8 Cost Function

> [4.8.1 如何定義neural transfer Cost Function？](#4.8.1)
> 
> [4.8.2 neural style transfer的訓練過程？](#4.8.2)

### 4.9 Content Cost Function

> [4.9.1 如何定義content loss function？](#4.9.1)

### 4.10 Style Cost Function

> [4.10.1 如何定義一張圖片的風格特徵？](#4.10.1)
> 
> [4.10.2 如何量化一張圖片的風格特徵？](#4.10.2)
> 
> [4.10.3 如何定義style loss function？](#4.10.3)
> 
> [4.10.4 如何讓風格特徵融合得更自然？](#4.10.4)

### 4.11 1D and 3D Generalizations

> [4.11.1 如何將2D圖片樣本使用的CNN模型應用到1D的數據上？](#4.11.1)
> 
> [4.11.2 如何將2D圖片樣本使用的CNN模型應用到3D的數據上？](#4.11.2)

---

## Answers:

<h3 id="4.1.1">4.1.1 理解face verification vs face recognition？</h3>

> * recognition 比 verification 難。
>
> * verification:
>
> > * inpuut: image & ID
> > * output: 1:1 match (這個ID是否這張image匹配)
> 
> * recognition:
> 
> > * inpuut: image & K個ID
> > * output: 1:K match (這個ID是否跟這K張image其中一張匹配)


<h3 id="4.2.1">4.2.1 什麼是one shot learning problem？</h3>

> ![37](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/37.png)
> 
> * 只給一張圖片就要讓模型可以具有識別能力。
> * 例子:
>
> > * feature: 4張照片
> > * labels: 4 ID
> > * model: 要從一張新照片卻判別是哪個ID
> > * 要如何從一個樣本就能成功訓練模型 ?

<h3 id="4.2.2">4.2.2 為什麼不能使用CNN？</h3>

> * 分類問題，通常是用CNN+softmax，但這樣的架構必須要有大量的training sample，才可以學會判別每個類別的feature，但現在的問題是training sample是有限的。
> * 每次要讓這個model可以處理新的分類時，就要需要再訓練一次這個model。

<h3 id="4.2.3">4.2.3 如何用similarity function 解決one shot learning problem？</h3>

> * 用新圖跟單張樣本圖做對比，利用similarity function來計算兩圖之間的差異度，當差異度小到一定的範圍內，則認為這兩張圖是指向同個人，這樣就完成verification的能力。
> * 用新圖跟所有樣本圖做一一對比，找出差異度最小的，這樣就完成recognition的能力。

<h3 id="4.3.1">4.3.1 什麼是Siamese network？</h3>

> ![38](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/38.png)
> 
> * 通過Siamese network來找出最佳的similarity function。
> 
> > * Siamese network會找出一組encoding fucntion對不同的輸入圖片進行encoding，我們目標希望兩張類似的圖片，encoding出的結果其差異度要夠小，這個encoding fucntion也就是我們想要找的similarity function。
> 
> * input: image
> * ouput: vector (encoding image)
> * 利用vector間的L2 distance來計算兩張圖的差異度。

<h3 id="4.4.1">4.4.1 理解triplet loss的核心部分？</h3>

> ![39](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/39.png)
> 
> * 定義triplet loss，確保Siamese network可以找出最佳encoding fucntion。
> * 核心部分: 訓練目標，anchor跟postive的差異度應該要小於anchor跟negative的差異度。

<h3 id="4.4.2">4.4.2 margin的必要性？</h3>

> > * 避免模型將encoding function訓練成f(img)=0。

<h3 id="4.4.3">4.4.3 為什麼要加入max函數設計？</h3>

> ![40](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/40.png)
> 
> * 當核心公式<0時，此時loss=0，也就是說在進行反向傳遞時，不會再更新參數值，因為當前參數值是符合我們的需求，反之，當核心公式>0，此時loss>0，進行反向傳遞時，就會更新參數值，好用來滿足我們的需求。

<h3 id="4.4.4">4.4.4 訓練時，一個identity需要有多個樣本？</h3>

> * 我們需要(A,P)來計算核心公式來進行訓練，但在測試時，一個identity只有一張圖片是沒問題的。

<h3 id="4.4.5">4.4.5 訓練時，為什麼不能隨機選擇圖片A，P，N？</h3>

> * 隨機選擇圖片，可能讓d(A,N)變得很大，遠大於d(A,P)，模型可能根本就不需要學習就可以滿足，之後若遇到很相似但是不同人的圖片，就會判別失敗，因此，我們要盡量選擇d(A,P)跟d(A,N)相近的值，也就是我們盡量找很相似但是不同人的圖片來訓練模型。

<h3 id="4.5.1">4.5.1 如何透過binary classification來實現Face verification / recognition？</h3>

> ![42](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/42.png)
> 
> * 將人臉識別變成一個簡單的二分類問題。
> 
> * 步驟:
> 
> > * 先構建Siamese network，可以將一張圖片進行encoding。
> > * 將兩張encoding後的值，丟入一個binary classifer，進行linear combination function跟sigmoid的計算，產生出一個0~1的機率值。
> > * 接近1，表示為同一人，接近0，表示為不同人。

<h3 id="4.6.1">4.6.1 什麼是Neural style transfer？</h3>

> 輸入兩張圖給模型，分別是content image跟style image，然後生成新圖，將style融合到content中。關鍵點在於圖片經過模型時，每一層layer所找出的feature是各不相同的，Neural style transfer分別對這些feature進行融合處理。

<h3 id="4.7.1">4.7.1 deep CNN在不同layer上學到什麼？</h3>

> ![43](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/43.png)
> 
> * 層數越深所學到的特徵越複雜，離真實世界的物體越相近。

<h3 id="4.7.2">4.7.2 如何理解多個“nine image patches“？</h3>

> ![44](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/44.png)
> 
> * unit's activation
> 
> > * 點對點相乘後在進行相加，經過sigmoid接近1，就稱為這個unit被activation，意味著這個unit對應的filter跟這個unit所對應的圖片位置，發現與filter相似的特徵。
> 
> * nine image patches
> 
> > * 每一格是3x3的圖片格，就是一個3x3的filter。
> > * 多個3x3的圖片格，就多個filter。
> > * 每個filter是用來尋找不同的特徵，如直線，橫線，斜線...。

<h3 id="4.8.1">4.8.1 如何定義neural transfer Cost Function？</h3>

> ![45](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/45.png)
> 
> * G與C的內容相似程度。
> * G與S的風格相似程度。
> * 兩個超參數，可以控制這兩個圖片融合的比例權重。 

<h3 id="4.8.2">4.8.2 neural style transfer的訓練過程？</h3>

> ![46](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/46.png)
> 
> * input: contnet image & style image。
> * weight: random init。
> * output: G
> 
> > * 其實weight在這model終就是G本身。
> 
> * 隨機初始化的weight丟入模型中，在經過neural transfer Cost Function肯定會產生很大值，因為G跟C跟S差異很大，經過gradient descent反向傳播後，會大幅修正weight內容。

<h3 id="4.9.1">4.9.1 如何定義content loss function？</h3>

> ![47](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/47.png)
> 
> ![48](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/48.png)
> 
> * 內容的特徵的表現，不會在淺層也不會在很深層，通常是在中間層進行被找出來。
> * 內容損失的計算，將C圖與G圖分別丟入CNN模型，然後在CNN模型選定一個中間層，提取相對應的特徵值分別計算其相似度。

<h3 id="4.10.1">4.10.1 如何定義一張圖片的風格特徵？</h3>

> ![49](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/49.png)
> 
> * 先決定好一層layer的output tensor(w,h,c)。
> * 本層的風格特徵就是該tensor中每兩個channe之間的correlation。
> 
> > * 也就是說當一個channel所對應的特徵出現在圖片時，另外一個channel所出現的特徵也出現在圖片的機率。
> > * correlation越高，兩種特徵同時出現的次數越多。
> 
> * 一張圖片在某一層的風格特徵，就是不同的channel的特徵同時出現在圖片的機率。

<h3 id="4.10.2">4.10.2 如何量化一張圖片的風格特徵？</h3>

> ![50](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/50.png)
> 
> * style matrix (gram matrix)
> 
> > * 一個tensor(w,h,c)，每個channel都是一個matrix(w,h)
> > * 讓兩個chnnel中的matrix所對應的值，點點相乘最後再相加，就得到2個channel間correlation值。
> > * 讓任兩個channel之間都做這樣的計算，會得一個style matrix。

<h3 id="4.10.3">4.10.3 如何定義style loss function？</h3>

> ![51](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/51.png)
> 
> * 風格損失的計算，將C圖與G圖分別丟入CNN模型，然後在CNN模型選定一個中間層，提取相對應的特徵值分別計算其style matrix。
> * 再針對對應style matrix計算其相似度。

<h3 id="4.10.4">4.10.4 如何讓風格特徵融合得更自然？</h3>

> * 讓C圖與G圖分別丟入CNN模型，然後在CNN模型選定多個中間層，分別做style相似度計算。
> * 每一層style相似度都擁有自己的權重，乘上自己的權重在相加求和。

<h3 id="4.11.1">4.11.1 如何將2D圖片樣本使用的CNN模型應用到1D的數據上？</h3>

> ![52](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/52.png)
> 
> * 1D 數據: 1行的input matrix。
> * 1D fliter: 1行的filter matrix。

<h3 id="4.11.2">4.11.2 如何將2D圖片樣本使用的CNN模型應用到3D的數據上？</h3>

> ![53](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Asset/53.png)
>
> * 3D 數據: w,h,t,c。(多了時間，也就是將圖片數據變成影片數據)
> * 3D fliter: fw,fh,ft,fc。 
