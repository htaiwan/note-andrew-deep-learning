## Assginemt 4 - Optimization Methods

### [1 - Gradient Descent](#1)

### [2 - Mini-Batch Gradient descent](#2)

### [3 - Momentum](#3)

### [4 - Adam](#4)

### [5 - Model with different optimization algorithms](#5)
>
> [5.1 - Mini-batch Gradient descent](#5.1)
> 
> [5.2 - Mini-batch gradient descent with momentum](#5.2)
> 
> [5.3 - Mini-batch with Adam mode](#5.3)
> 
> [5.4 - Summary](#5.4)

---

### [Assginemt 4](https://github.com/htaiwan/note-andrew-deep-learning/blob/master/Assignment/Course2/2-4.ipynb)